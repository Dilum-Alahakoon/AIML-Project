{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Group Pipeline"
      ],
      "metadata": {
        "id": "yG3W_pVEC41S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "fDLmUIUcDCcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "yrh2yrZLDI32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CnqYffwsDQHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the dataset\n",
        "data=pd.read_csv(\"https://raw.githubusercontent.com/Dilum-Alahakoon/AIML-Project/refs/heads/main/data/raw/post_pandemic_remote_work_health_impact_2025.csv\")\n",
        "\n",
        "# Converting to a dataframe\n",
        "data_df=pd.DataFrame(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "1XAai6i-DReE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first five rows\n",
        "data_df.head()"
      ],
      "metadata": {
        "id": "wGWOwB9SDTAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the last five rows\n",
        "data_df.tail()"
      ],
      "metadata": {
        "id": "Zga5P5lLDUml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of the dataset\n",
        "\n",
        "print(f\"Shape of the dataset: {data_df.shape}\")\n",
        "print(f\"Number of rows : {data_df.shape[0]}\")\n",
        "print(f\"Number of rows : {data_df.shape[1]}\")"
      ],
      "metadata": {
        "id": "X44LJ8gCDYXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Information\n",
        "data_df.info()"
      ],
      "metadata": {
        "id": "2RmWG4maDaaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive statistics of numerical data in dataset\n",
        "data_df.describe()"
      ],
      "metadata": {
        "id": "n4LFkhB3DcNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['Mental_Health_Status'].value_counts()"
      ],
      "metadata": {
        "id": "GYbNxkH2DeT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ax = data_df.hist(\n",
        "    bins=50,\n",
        "    figsize=(20, 10),\n",
        "    color='skyblue',\n",
        "    edgecolor='black',\n",
        "    grid=False\n",
        ")\n",
        "\n",
        "\n",
        "for a in ax.ravel():\n",
        "    a.set_facecolor(\"#f9f9f9\")\n",
        "    a.grid(True, linestyle='--', alpha=0.5)\n",
        "    a.tick_params(axis='x', labelsize=10)\n",
        "    a.tick_params(axis='y', labelsize=10)\n",
        "    a.set_title(a.get_title(), fontsize=12, fontweight='bold')\n",
        "\n",
        "\n",
        "plt.suptitle(\"Feature Distributions\", fontsize=20, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ruwm-xbFDgg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features=data_df.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_features=data_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"Categorical Features: {categorical_features}\\n\")\n",
        "print(f\"Numerical Features: {numerical_features}\")"
      ],
      "metadata": {
        "id": "61hzOzc0DiX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in categorical_features:\n",
        "  print(data_df[col].value_counts())\n",
        "  print()"
      ],
      "metadata": {
        "id": "TGvxCfpgDkqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "for col in categorical_features:\n",
        "  plt.figure(figsize=(8,4))\n",
        "  sns.countplot(y=col,data=data_df,order=data_df[col].value_counts().index,palette='viridis')\n",
        "  plt.title(f\"Distribution of {col}\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "aJ9iV1bADrZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Missing Data"
      ],
      "metadata": {
        "id": "li5i0SpBDuEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying the missing data\n",
        "\n",
        "data_df.isnull().sum()"
      ],
      "metadata": {
        "id": "QeqYDEwIEASi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heatmap for missing values\n",
        "\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(data_df.isnull(),cbar=False)\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "missing_counts=data_df.isnull().sum().sort_values(ascending=False)\n",
        "print(missing_counts[missing_counts>0])"
      ],
      "metadata": {
        "id": "rapR_PJtECJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar graph for the missing values\n",
        "\n",
        "missing_counts=data_df.isnull().sum()\n",
        "missing_cols=missing_counts[missing_counts>0].sort_values(ascending=False)\n",
        "\n",
        "if len(missing_cols)==0:\n",
        "  print(\"No missing values in the dataset!\")\n",
        "else:\n",
        "  plt.figure(figsize=(10,6))\n",
        "  sns.barplot(x=missing_cols.index, y=missing_cols.values,palette='viridis')\n",
        "  plt.xticks(rotation=45,ha='right')\n",
        "  plt.ylabel('Number of Missing Values')\n",
        "  plt.title('Missing Values Count by Feature')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "YS2PPo3dED6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the missing values of the target variable(Mental Helth Status)\n",
        "\n",
        "data_df.dropna(subset=['Mental_Health_Status'],inplace=True)"
      ],
      "metadata": {
        "id": "VJvN-EOHEF-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.isnull().sum()"
      ],
      "metadata": {
        "id": "PRVMxQTXEIAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data shape after removing the missing values of the target variable\n",
        "\n",
        "print(f\"\\nNumber of rows & columns after removing the missing values of the target variable: {data_df.shape}\")"
      ],
      "metadata": {
        "id": "tHdUeX80ELfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hetamap after removing the missing values of the target variable\n",
        "\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(data_df.isnull(),cbar=False)\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "missing_counts=data_df.isnull().sum().sort_values(ascending=False)\n",
        "print(missing_counts[missing_counts>0])"
      ],
      "metadata": {
        "id": "cfOZ_ipuEQXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar graph after removing the target variable (Mental Health Status) missing values\n",
        "\n",
        "missing_counts=data_df.isnull().sum()\n",
        "missing_cols=missing_counts[missing_counts>0].sort_values(ascending=False)\n",
        "\n",
        "if len(missing_cols)==0:\n",
        "  print(\"No missing values in the dataset!\")\n",
        "else:\n",
        "  plt.figure(figsize=(10,6))\n",
        "  sns.barplot(x=missing_cols.index, y=missing_cols.values,palette='viridis')\n",
        "  plt.xticks(rotation=45,ha='right')\n",
        "  plt.ylabel('Number of Missing Values')\n",
        "  plt.title('Missing Values Count by Feature')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "cmrm8a9CESJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After removing the missing values of the target variable there are 203 missing values in the Phisical Health Issues because of that we need to handle that missing values"
      ],
      "metadata": {
        "id": "HB13489mEWl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the count of the physical health issues count\n",
        "\n",
        "data_df['Physical_Health_Issues'].value_counts()"
      ],
      "metadata": {
        "id": "sVAvX8kcEUUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=data_df.drop(labels=['Physical_Health_Issues'],axis=1)\n",
        "y=data_df['Mental_Health_Status']"
      ],
      "metadata": {
        "id": "yGq8AZ8FEbnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features=X.select_dtypes(include=[np.number]).columns.tolist()"
      ],
      "metadata": {
        "id": "MJlIozhbEd3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a new dataframe only using numerical features\n",
        "test_dataframe=pd.DataFrame(X[numerical_features])"
      ],
      "metadata": {
        "id": "PQ9hj_xEEfZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to this dataframe attaching the target variable as Physical_Health_Issues\n",
        "test_dataframe['target']=data_df['Physical_Health_Issues']"
      ],
      "metadata": {
        "id": "so9QafkyFBOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataframe.head()"
      ],
      "metadata": {
        "id": "d7ijTD_YEhBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating the missing values in the new dataframe\n",
        "# here in the data frame in the target variable there are 208 missing values\n",
        "\n",
        "test_dataframe.isnull().sum()"
      ],
      "metadata": {
        "id": "QiU2fw_hEiaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here i split the newly formed dataset into two parts known part contain the rows with not missing values in the target variable and missing part it contains rows with missing values in the target variable\n",
        "\n",
        "known=test_dataframe[test_dataframe['target'].notna()]\n",
        "missing=test_dataframe[test_dataframe['target'].isna()]"
      ],
      "metadata": {
        "id": "-HeWZFwJEl5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "known.head()"
      ],
      "metadata": {
        "id": "xY8c9YAdEnuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here in the known column we are split into input and and predicting variable\n",
        "\n",
        "mis_X=known.drop('target',axis=1)\n",
        "mis_y=known['target']"
      ],
      "metadata": {
        "id": "nwj-cmpUFTX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using randomforest classifier to train a model using abobe splitted data\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model=RandomForestClassifier(random_state=42)\n",
        "model.fit(mis_X,mis_y)"
      ],
      "metadata": {
        "id": "ld-jFRZEFVcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seperating the unknown part and drop the target variable\n",
        "X_unknown=missing.drop('target',axis=1)\n",
        "\n",
        "# predicting the values for X_unknown\n",
        "predicted=model.predict(X_unknown)"
      ],
      "metadata": {
        "id": "OjOVumWXFXRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# attaching the predicted part to the dataframe\n",
        "\n",
        "test_dataframe.loc[test_dataframe['target'].isna(),'target']=predicted"
      ],
      "metadata": {
        "id": "dacFVK6DFaG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.drop('Physical_Health_Issues',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "mR8NxOdzFb0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['Physical_Health_Issues']=test_dataframe['target']"
      ],
      "metadata": {
        "id": "KSSKBcb0Feud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['Physical_Health_Issues'].value_counts()"
      ],
      "metadata": {
        "id": "GXDxkYxbFiFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After removing the missing values\n",
        "\n",
        "data_df.isnull().sum()\n",
        "\n"
      ],
      "metadata": {
        "id": "CPiCZa1uGH5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Creation"
      ],
      "metadata": {
        "id": "fANZFU6IGTtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of the dataset: {data_df.shape}\")\n",
        "print(f\"Number of rows : {data_df.shape[0]}\")\n",
        "print(f\"Number of columns : {data_df.shape[1]}\")"
      ],
      "metadata": {
        "id": "4_AZ2z14camZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Viewing the basic details of the dataset\n",
        "if 'data_df' in locals():\n",
        "    print(\"\\n--- First 5 rows of the dataset: ---\")\n",
        "    print(data_df.head())\n",
        "\n",
        "    print(\"\\n--- Dataset Info (Columns, Data Types, Non-null counts): ---\")\n",
        "    data_df.info()"
      ],
      "metadata": {
        "id": "tZOsi3PNcezx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['Salary_Range'].value_counts()"
      ],
      "metadata": {
        "id": "X-06GrG0c0nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['Salary_Range_Clean'] = data_df['Salary_Range'].str.replace('$', '', regex=False).str.replace('K', '', regex=False).str.replace('+', '', regex=False)\n"
      ],
      "metadata": {
        "id": "J7XbvleTc45q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()\n"
      ],
      "metadata": {
        "id": "mKK4p7Twc9Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salary_split = data_df['Salary_Range_Clean'].str.split('-', expand=True)\n"
      ],
      "metadata": {
        "id": "Q2CezGLmdB_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_salary = pd.to_numeric(salary_split[0])\n"
      ],
      "metadata": {
        "id": "HuzAGLtudFQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_salary = pd.to_numeric(salary_split[1]).fillna(min_salary)\n"
      ],
      "metadata": {
        "id": "L7UulAjldI-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['Average_Salary'] = (min_salary + max_salary) / 2\n"
      ],
      "metadata": {
        "id": "SGemexVwdKd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()\n"
      ],
      "metadata": {
        "id": "nsMckOKCdMld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the original and temporary salary columns as they are no longer needed\n",
        "data_df = data_df.drop(columns=['Salary_Range', 'Salary_Range_Clean'])\n",
        "\n",
        "print(\"Successfully created 'Average_Salary' feature (with error fixed).\")\n",
        "print(data_df.head())"
      ],
      "metadata": {
        "id": "tSuhQGlvdPQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['Physical_Issue_Count'] = data_df['Physical_Health_Issues'].apply(lambda x: 0 if x == 'None' else len(x.split(';')))\n"
      ],
      "metadata": {
        "id": "51iffhDodUmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSuccessfully created 'Physical_Issue_Count' feature.\")\n"
      ],
      "metadata": {
        "id": "qJyveETldYJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_df[['Physical_Health_Issues', 'Physical_Issue_Count']].head())\n"
      ],
      "metadata": {
        "id": "oTVMZdbfdZqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_df.head(5))\n"
      ],
      "metadata": {
        "id": "8qUPGTtWdbsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.columns\n"
      ],
      "metadata": {
        "id": "w4ijjIpAdf-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()\n"
      ],
      "metadata": {
        "id": "yQgtq_sedi5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding_Categorical_Varables"
      ],
      "metadata": {
        "id": "6cZaBcuadlRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib"
      ],
      "metadata": {
        "id": "NADO8-L0dtI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(data_df.head())\n"
      ],
      "metadata": {
        "id": "LRWShBJEdzDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features=data_df.select_dtypes(include=['object']).columns.tolist()\n"
      ],
      "metadata": {
        "id": "8Jtuz1Vsd1Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# information about categorical labels\n",
        "for col in categorical_features:\n",
        "  print(data_df[col].value_counts())\n",
        "  print()"
      ],
      "metadata": {
        "id": "K7P0h0Cvd6gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to choose categorical columns for encode using onehot encoding and label encoding\n",
        "\n"
      ],
      "metadata": {
        "id": "4ANTw6k_eCJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_columns=['Physical_Health_Issues','Work_Arrangement','Job_Role','Region','Gender']\n",
        "label_columns=['Burnout_Level']"
      ],
      "metadata": {
        "id": "GbLuGlAFeDna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing onehot encoder from sklearn\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "onehot=OneHotEncoder(sparse_output=False)\n",
        "\n",
        "\n",
        "# fitting the data to onehot encoder\n",
        "onehot.fit(data_df[one_hot_columns])"
      ],
      "metadata": {
        "id": "6GlagBPceGpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the data\n",
        "transformed_onehot_data=onehot.transform(data_df[one_hot_columns])"
      ],
      "metadata": {
        "id": "uPukR9TqeIbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping the onehot columns from the origianl dataset\n",
        "data_df.drop(one_hot_columns,axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "g7qXRK20ePBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature names from onehot.categories_\n",
        "onehot_feature_names = []\n",
        "for i, col in enumerate(one_hot_columns):\n",
        "  for cat in onehot.categories_[i]:\n",
        "    onehot_feature_names.append(f'{col}_{cat}')\n",
        "\n",
        "# Create a DataFrame from the transformed data\n",
        "transformed_one_hot_df = pd.DataFrame(transformed_onehot_data, columns=onehot_feature_names, index=data_df.index)\n",
        "\n",
        "# Concatenate the original DataFrame and the new one-hot encoded DataFrame\n",
        "data_df = pd.concat([data_df, transformed_one_hot_df], axis=1)"
      ],
      "metadata": {
        "id": "98H5XRyOeQiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# doing the label encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()"
      ],
      "metadata": {
        "id": "geIiy_0DeTTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.head()"
      ],
      "metadata": {
        "id": "CieeD28PeVea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#One Hot Encoded Histograms\n",
        "for col in onehot_feature_names:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.histplot(data_df[col], bins=2, kde=False)\n",
        "    plt.title(f'Histogram of {col} (One-Hot Encoded)')\n",
        "    plt.xlabel(\"Value (0 = Absent, 1 = Present)\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rwUnnyLTeXGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum each one-hot column\n",
        "category_counts = data_df[onehot_feature_names].sum().sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=category_counts.index, y=category_counts.values)\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.title(\"Frequency of Categories (One-Hot Encoded)\")\n",
        "plt.xlabel(\"Category\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KkHoFGBjeZ9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Label Encoded Histogram\n",
        "for col in label_columns:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.histplot(data_df[col], bins=len(data_df[col].unique()), kde=False)\n",
        "    plt.title(f'Histogram of {col} (Label Encoded)')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3kHg7_lJegPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Feature Selection"
      ],
      "metadata": {
        "id": "W-cAHAFgejGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "E3UBWdZnerQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the first 5 rows of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(data_df.head())\n",
        "\n",
        "# Getting a summary of the dataset's information\n",
        "print(\"\\nDataset Info:\")\n",
        "data_df.info()"
      ],
      "metadata": {
        "id": "Nmjj7MkWevBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the 'Survey_Date' column\n",
        "data_df = data_df.drop('Survey_Date', axis=1)"
      ],
      "metadata": {
        "id": "sxMY6Fa7exkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the 'Industry' column using one-hot encoding\n",
        "# This creates multiple columns based on the industry type instead of a single column\n",
        "data_df = pd.get_dummies(data_df, columns=['Industry'], drop_first=True)"
      ],
      "metadata": {
        "id": "6MbR84qye7Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding our target variable, the 'Mental_Health_Status' column\n",
        "# Here, text values like 'Anxiety', 'Depression' are converted to numbers like 0, 1, 2\n",
        "label_encoder = LabelEncoder()\n",
        "data_df['Mental_Health_Status'] = label_encoder.fit_transform(data_df['Mental_Health_Status'])"
      ],
      "metadata": {
        "id": "OUWie-h_fCJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Viewing the dataset after preprocessing\n",
        "print(\"\\nDataset after preprocessing:\")\n",
        "print(data_df.head())\n",
        "data_df.info()"
      ],
      "metadata": {
        "id": "0EQuY7gHfG4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating the features (X) and the target variable (y)\n",
        "X = data_df.drop('Mental_Health_Status', axis=1)\n",
        "y = data_df['Mental_Health_Status']"
      ],
      "metadata": {
        "id": "GG8so-2ZfKyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating the features (X) and the target variable (y)\n",
        "X = data_df.drop('Mental_Health_Status', axis=1)\n",
        "y = data_df['Mental_Health_Status']\n",
        "\n",
        "# Select only the numerical columns for correlation analysis, excluding the 'Burnout_Level'\n",
        "numerical_cols_for_corr = X.select_dtypes(include=np.number).columns\n",
        "\n",
        "# Creating the correlation matrix of the numerical features\n",
        "# Exclude 'Burnout_Level' for now as it will be label encoded later\n",
        "corr_matrix = X[numerical_cols_for_corr].corr().abs()\n",
        "\n",
        "# Selecting the upper triangle of the correlation matrix\n",
        "# Because the matrix is symmetric, we only need to check one half\n",
        "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# Identifying features with a correlation greater than 0.90 to add to the 'to_drop' list\n",
        "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.90)]\n",
        "\n",
        "print(f\"\\nFeatures to drop due to high correlation ({len(to_drop)}):\")\n",
        "print(to_drop)\n",
        "\n",
        "# Dropping these features from the DataFrame X\n",
        "X_filtered = X.drop(columns=to_drop)\n",
        "\n",
        "# Label encode the 'Burnout_Level' column in X_filtered\n",
        "label_encoder = LabelEncoder()\n",
        "X_filtered['Burnout_Level'] = label_encoder.fit_transform(X_filtered['Burnout_Level'])\n",
        "\n",
        "print(f\"\\nNumber of remaining features after dropping: {X_filtered.shape[1]}\")\n",
        "\n",
        "# Visualizing the correlation matrix with a heatmap (Optional)\n",
        "# Include 'Burnout_Level' now that it's encoded\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(X_filtered.corr(), cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap of Features (After Dropping Highly Correlated)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TlaO26EFfU96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_filtered.info()"
      ],
      "metadata": {
        "id": "jUjse1nqgACD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'X_filtered' and 'y' are available from the previous steps\n",
        "\n",
        "# Creating a RandomForestClassifier model\n",
        "# n_estimators is the number of trees in the forest\n",
        "# random_state ensures that the results are reproducible\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Training the model using our filtered features and the target variable\n",
        "model.fit(X_filtered, y)\n",
        "\n",
        "# Getting the feature importances from the trained model\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Putting the feature importances into a pandas DataFrame and sorting them\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': X_filtered.columns,\n",
        "    'importance': importances\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances (Ranked):\")\n",
        "print(feature_importance_df)\n",
        "\n",
        "# Visualizing the top 15 most important features\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance_df.head(15), palette='viridis', hue='feature', legend=False)\n",
        "plt.title('Top 15 Most Important Features (from Random Forest)')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Selecting the list of the top 15 most important features\n",
        "top_15_features = feature_importance_df.head(15)['feature'].tolist()\n",
        "\n",
        "print(\"\\nFinal list of the selected top 15 features:\")\n",
        "print(top_15_features)"
      ],
      "metadata": {
        "id": "3IO6oqOYgMBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'X_filtered' and 'top_15_features' are available from the previous steps\n",
        "\n",
        "# We have the list of top 15 features selected by Random Forest from the previous step\n",
        "# top_15_features = ['Burnout_Level', 'Work_Life_Balance_Score', 'Social_Isolation_Score', ...]\n",
        "\n",
        "# Now, from the correlation-filtered DataFrame (X_filtered),\n",
        "# we create a new DataFrame by selecting only those most important features.\n",
        "# This will be our final feature set.\n",
        "X_final = X_filtered[top_15_features]\n",
        "\n",
        "# The target variable (y) remains the same.\n",
        "\n",
        "#--------------------------------------------------------------\n",
        "# From now on, you will use X_final and y for your future steps\n",
        "#--------------------------------------------------------------\n",
        "\n",
        "print(\"Final Features to be used in the Model (X_final):\")\n",
        "print(f\"Number of features: {X_final.shape[1]}\")\n",
        "print(X_final.head())\n",
        "\n",
        "print(\"\\nTarget Variable (y):\")\n",
        "print(y.head())"
      ],
      "metadata": {
        "id": "nKxZ1URRga_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zscore_Scaling"
      ],
      "metadata": {
        "id": "2i_AoLHwgg9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Columns to scale (continuous numeric ones)\n",
        "cols_to_scale = [\"Age\", \"Hours_Per_Week\", \"Average_Salary\"]\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the selected columns\n",
        "data_df[cols_to_scale] = scaler.fit_transform(data_df[cols_to_scale])\n",
        "\n",
        "print(data_df.head())"
      ],
      "metadata": {
        "id": "yTPBzPxSl-jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outlier__Removal"
      ],
      "metadata": {
        "id": "Jzrn9tx6mBju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "7s8Tb6GMmrrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selecting numerical features\n",
        "numerical_features=data_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"Numerical features: \\n{numerical_features}\")"
      ],
      "metadata": {
        "id": "3CFwUu2ymZup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
        "\n",
        "n = len(numerical_features)\n",
        "cols = 3\n",
        "rows = math.ceil(n / cols)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n",
        "\n",
        "\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(numerical_features):\n",
        "    sns.boxplot(y=df[col], ax=axes[i], color=sns.color_palette(\"Set2\")[i % 8])\n",
        "    axes[i].set_title(f\"{col} Distribution\", fontsize=12, fontweight=\"bold\")\n",
        "    axes[i].set_xlabel(\"\")\n",
        "    axes[i].tick_params(axis='y', labelsize=10)\n",
        "\n",
        "\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "JdGAHLmTmcOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JIZ88kE2mlK-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}